{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dff793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e786c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded ./raw_zips/Copy OSA - Parity 2.0 1/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Misc)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Startek_Compassion- Gift Donation. BEG/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Sales U - Closing Conclusion/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Startek_Telco_Visual_BEG_Demo /data.json (dict)\n",
      "✓ Loaded ./raw_zips/Arise_Disney Resorts Reservations - Booking Multiple Rooms Package Reservation ~ ADV 2/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Arise_Carnival DS - Sales Inquiries - Booking created ~ BEG/data.json (dict)\n",
      "✓ Loaded ./raw_zips/H21 Health insurance coverage question – Step 1 (Opening)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/TRAINEE STARTS HERE_ How did you do_ View your Playback!  /data.json (dict)\n",
      "✓ Loaded ./raw_zips/H21 Health insurance coverage question – Step 2 (Gather Information)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/UCSL eApp – State Specifics/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Demo sym - Prescription Request__BEG - Screen Navigation/data.json (dict)\n",
      "✓ Loaded ./raw_zips/OSA - Parity 2.0/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Sales U - Closing Practice 4 - Empathy Close/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Health Insurance (advanced) Adjust existing payment plan _ Part 1 of 2 Discover need and propose solution/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Sales U - Closing Practice 6 - Urgency Close/data.json (dict)\n",
      "✓ Loaded ./raw_zips/FINANCIAL _ B1 Zelle Payment issue - extreme SHORTENED/data.json (dict)\n",
      "✓ Loaded ./raw_zips/BN_ORDER_LOST_REPLACE_ADV/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Arise_Disney Resorts Reservations - Booking Multiple Rooms Package Reservation ~ ADV/data.json (dict)\n",
      "✓ Loaded ./raw_zips/C2 Car insurance claim - FNOL – ADVANCED/data.json (dict)\n",
      "✓ Loaded ./raw_zips/H21 Health insurance coverage question – (Real human voice) (1)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Startek_Compassion- Credit Card Payment. BEG/data.json (dict)\n",
      "✓ Loaded ./raw_zips/United Flight Change/data.json (dict)\n",
      "✓ Loaded ./raw_zips/SymTrain - Sym Playback/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Startek_Walmart - Demo - 2025/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Dick_s Sporting Goods - Kayak Pick up/data.json (dict)\n",
      "✓ Loaded ./raw_zips/AT&T - Transition to Sales- Assessment for Work from Home Agent/data.json (dict)\n",
      "✓ Loaded ./raw_zips/OSA - Parity/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Arise_Carnival CS - Mock Call Assessment (Tollgate)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Salesforce Quote Tool/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Startek_Compassion- Credit Card Payment. ADV/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Copy TLE eApp-Final Producer Signature 2/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Learning Style Assessment Simulation_NAV/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Zinus Knowledge Check 6 Version 2/data.json (dict)\n",
      "✓ Loaded ./raw_zips/H21 Health insurance coverage question – (Real human voice)/data.json (dict)\n",
      "✓ Loaded ./raw_zips/Raising a Commitment/data.json (dict)\n",
      "✓ Loaded ./raw_zips/SymTrain - Overview/data.json (dict)\n",
      "✓ Loaded ./raw_zips/BN_CANCEL_RELEASED_ORDER_ADV/data.json (dict)\n",
      "✓ Loaded ./raw_zips/C2 Car insurance claim - FNOL – Step 1 (Opening)/data.json (dict)\n",
      "✓ Loaded ./data/jsons/Dick_s Sporting Goods - Kayak Pick up.json (dict)\n",
      "✓ Loaded ./data/jsons/H21 Health insurance coverage question – Step 2 (Gather Information).json (dict)\n",
      "✓ Loaded ./data/jsons/Arise_Carnival CS - Mock Call Assessment (Tollgate).json (dict)\n",
      "✓ Loaded ./data/jsons/AT&T - Transition to Sales- Assessment for Work from Home Agent.json (dict)\n",
      "✓ Loaded ./data/jsons/Sales U - Closing Conclusion.json (dict)\n",
      "✓ Loaded ./data/jsons/Sales U - Closing Practice 6 - Urgency Close.json (dict)\n",
      "✓ Loaded ./data/jsons/Startek_Compassion- Credit Card Payment. BEG.json (dict)\n",
      "✓ Loaded ./data/jsons/Health Insurance (advanced) Adjust existing payment plan _ Part 1 of 2 Discover need and propose solution.json (dict)\n",
      "✓ Loaded ./data/jsons/Arise_Disney Resorts Reservations - Booking Multiple Rooms Package Reservation ~ ADV.json (dict)\n",
      "✓ Loaded ./data/jsons/Copy TLE eApp-Final Producer Signature 2.json (dict)\n",
      "✓ Loaded ./data/jsons/Misc).json (dict)\n",
      "✓ Loaded ./data/jsons/H21 Health insurance coverage question – (Real human voice) (1).json (dict)\n",
      "✓ Loaded ./data/jsons/SymTrain - Sym Playback.json (dict)\n",
      "✓ Loaded ./data/jsons/Startek_Compassion- Credit Card Payment. ADV.json (dict)\n",
      "✓ Loaded ./data/jsons/H21 Health insurance coverage question – (Real human voice).json (dict)\n",
      "✓ Loaded ./data/jsons/SymTrain - Overview.json (dict)\n",
      "✓ Loaded ./data/jsons/OSA - Parity 2.0.json (dict)\n",
      "✓ Loaded ./data/jsons/Salesforce Quote Tool.json (dict)\n",
      "✓ Loaded ./data/jsons/Learning Style Assessment Simulation_NAV.json (dict)\n",
      "✓ Loaded ./data/jsons/C2 Car insurance claim - FNOL – ADVANCED.json (dict)\n",
      "✓ Loaded ./data/jsons/Zinus Knowledge Check 6 Version 2.json (dict)\n",
      "✓ Loaded ./data/jsons/Raising a Commitment.json (dict)\n",
      "✓ Loaded ./data/jsons/OSA - Parity.json (dict)\n",
      "✓ Loaded ./data/jsons/Arise_Carnival DS - Sales Inquiries - Booking created ~ BEG.json (dict)\n",
      "✓ Loaded ./data/jsons/Demo sym - Prescription Request__BEG - Screen Navigation.json (dict)\n",
      "✓ Loaded ./data/jsons/Startek_Walmart - Demo - 2025.json (dict)\n",
      "✓ Loaded ./data/jsons/BN_CANCEL_RELEASED_ORDER_ADV.json (dict)\n",
      "✓ Loaded ./data/jsons/Copy OSA - Parity 2.0 1.json (dict)\n",
      "✓ Loaded ./data/jsons/C2 Car insurance claim - FNOL – Step 1 (Opening).json (dict)\n",
      "✓ Loaded ./data/jsons/United Flight Change.json (dict)\n",
      "✓ Loaded ./data/jsons/FINANCIAL _ B1 Zelle Payment issue - extreme SHORTENED.json (dict)\n",
      "✓ Loaded ./data/jsons/UCSL eApp – State Specifics.json (dict)\n",
      "✓ Loaded ./data/jsons/Sales U - Closing Practice 4 - Empathy Close.json (dict)\n",
      "✓ Loaded ./data/jsons/H21 Health insurance coverage question – Step 1 (Opening).json (dict)\n",
      "✓ Loaded ./data/jsons/Startek_Compassion- Gift Donation. BEG.json (dict)\n",
      "✓ Loaded ./data/jsons/BN_ORDER_LOST_REPLACE_ADV.json (dict)\n",
      "\n",
      "Summary: 75 JSON files, 75 total records\n"
     ]
    }
   ],
   "source": [
    "# Store all loaded data\n",
    "all_data = []\n",
    "stats = defaultdict(int)\n",
    "\n",
    "root_directory = '.'\n",
    "\n",
    "for root, dirs, files in os.walk(root_directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Add metadata for tracking\n",
    "                data['_source_file'] = file_path\n",
    "                data['_source_dir'] = root\n",
    "                all_data.append(data)\n",
    "                \n",
    "                # Collect basic stats\n",
    "                stats['total_files'] += 1\n",
    "                stats['total_records'] += len(data) if isinstance(data, list) else 1\n",
    "                \n",
    "                print(f\"✓ Loaded {file_path} ({type(data).__name__})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nSummary: {stats['total_files']} JSON files, {stats['total_records']} total records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e10595",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioContentItems_extracted = []\n",
    "for item in all_data:\n",
    "    audioContentItems_extracted.append(item['audioContentItems'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22062282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Merge dialogue text\n",
    "merged_transcripts = []\n",
    "\n",
    "for items in audioContentItems_extracted:\n",
    "    # 1. Sort by sequenceNumber to ensure the conversation flow is correct\n",
    "    sorted_items = sorted(items, key=lambda x: x.get('sequenceNumber', 0))\n",
    "    \n",
    "    # 2. Format each line as \"Actor: Text\"\n",
    "    conversation_parts = []\n",
    "    for entry in sorted_items:\n",
    "        actor = entry.get('actor', 'Unknown')\n",
    "        \n",
    "        if actor == 'TRAINEE':\n",
    "            actor = 'Trainee'\n",
    "            \n",
    "        text = entry.get('fileTranscript', '')\n",
    "        conversation_parts.append(f\"{actor}: {text}\")\n",
    "    \n",
    "    # 3. Join all parts into one continuous string\n",
    "    full_transcript = \" \".join(conversation_parts)\n",
    "    merged_transcripts.append(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e10b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trainee: Thank you for calling Northwestern Mutual, Underwriting Case Management, this is ________. How may I help you? SYM: Hi! This is Norm Weston calling from John Barrett's office. This case for Marty Juana has taken forever, what is the underwriter waiting on? Trainee: Hi Norm, may I start with a policy number to get started? SYM: This case for Marty Juana has taken forever, what is the underwriter waiting on? The policy number is T195129. Trainee: Thank you, Norm. Give me one moment to pull up the case. Trainee: Thank you, Norm, I do have Marty's case pulled up here. You would like to talk about what will move this case to approval, is that correct? SYM: Yes, that is what I'm looking for. Trainee: Please give me a moment so I can review the details of the case and see what we can do to get this moving. SYM: Sounds good! Trainee: Thank you so much, I am all up to speed on this case. It looks like we are awaiting records from Dr. Smith's office and that a special authorization was sent to your office by the APS vendor to be reviewed. Do you have a copy of that completed form? SYM: No, I thought you sent it to the client and not just to me. Trainee: Thank you, Norm. Let me take a moment to review the notes from our APS vendor to determine where the document was sent. SYM: Please do, I hope you sent it to the client. Trainee: It doesn't appear that this was sent directly to the client, instead it was sent to your office to facilitate that request. I can have that resent to you right away. SYM: Ok, that is fine. Do you really need these records though? It's been a long time. Trainee: It does look from the notes that the underwriter does require these notes in order to reach the final decision, however, I will put in a note requesting for a risk review and the underwriter will reach out with that information. In the meantime, I'll get the form over to you to reduce any further delay. SYM: That sounds fair. Thank you. Trainee: Of course. Is there anything else I can assist you with? SYM: No, that was all. Trainee: Awesome, well thank you so much for putting your trust in Northwestern Mutual. Have a great rest of your day. SYM: Thank you, you as well!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc8d50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad28c155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f355e09e5276471a924ea1fabb2dcd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "extractor = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad107d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(transcript):\n",
    "    prompt = f\"\"\"<|user|>\n",
    "Read the following customer service transcript carefully.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript}\n",
    "\n",
    "TASK:\n",
    "1. The reason the customer called (what they need help with)\n",
    "2. The Steps the agent took to help them (as a list)\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"reason\": \"brief reason for the call\",\n",
    "  \"steps\": [\n",
    "    \"step 1\",\n",
    "    \"step 2\",\n",
    "    \"...\",\n",
    "    \"step N\"\n",
    "  ]\n",
    "}}\n",
    "<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b2437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript 1...\n",
      "--- Result 1 ---\n",
      " ```json\n",
      "{\n",
      "  \"reason\": \"Underwriter delay on Marty Juana's case\",\n",
      "  \"steps\": [\n",
      "    \"Trainee greets the caller and asks for the policy number\",\n",
      "    \"Trainee pulls up the case using the provided policy number\",\n",
      "    \"Trainee confirms the caller's request to expedite the case\",\n",
      "    \"Trainee reviews the case details and identifies the need for records from Dr. Smith's office and a special authorization\",\n",
      "    \"Trainee checks with the APS vendor and finds the authorization was sent to the caller's office\",\n",
      "    \"Trainee resends the authorization to the client's office\",\n",
      "    \"Trainee requests a risk review from the underwriter and informs the caller of the next steps\",\n",
      "    \"Trainee offers further assistance and thanks the caller for their trust in Northwestern Mutual\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing transcript 2...\n",
      "--- Result 2 ---\n",
      " ```json\n",
      "{\n",
      "  \"reason\": \"Customer wants to return a bed frame due to color preference and is within the return period.\",\n",
      "  \"steps\": [\n",
      "    \"Agent greeted the customer and asked for their name and email address.\",\n",
      "    \"Agent confirmed the customer's name and email.\",\n",
      "    \"Agent apologized for the inconvenience and offered to look at the return options.\",\n",
      "    \"Agent requested the order number to validate the purchase.\",\n",
      "    \"Agent confirmed the order details and delivery date.\",\n",
      "    \"Agent confirmed the customer was within the return period.\",\n",
      "    \"Agent explained the return process, including the need for the item to be returned in its original packaging.\",\n",
      "    \"Agent informed the customer that a prepaid return label would be sent and FedEx pickup arranged.\",\n",
      "    \"Agent stated that a refund would be issued within 2-3 business days after the item was received.\",\n",
      "    \"Agent offered further assistance and provided contact information for any future questions.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing transcript 3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing transcript \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m full_prompt = create_prompt(transcript)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m outputs = \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m generated_text = outputs[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m results.append({\u001b[33m\"\u001b[39m\u001b[33moriginal_transcript\u001b[39m\u001b[33m\"\u001b[39m: transcript,\n\u001b[32m     12\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mextraction\u001b[39m\u001b[33m\"\u001b[39m: generated_text})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/pipelines/base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/pipelines/base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/models/phi3/modeling_phi3.py:465\u001b[39m, in \u001b[36mPhi3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    446\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    447\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    448\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    450\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/models/phi3/modeling_phi3.py:388\u001b[39m, in \u001b[36mPhi3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    385\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    387\u001b[39m mask_function = create_causal_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.sliding_window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m create_sliding_window_causal_mask\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m causal_mask = \u001b[43mmask_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    398\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/masking_utils.py:883\u001b[39m, in \u001b[36mcreate_sliding_window_causal_mask\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     layer_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = \u001b[43m_preprocess_mask_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m early_exit:\n\u001b[32m    887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/transformers/masking_utils.py:723\u001b[39m, in \u001b[36m_preprocess_mask_arguments\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# Move the mask to correct device, and potentially switch dtype for efficiency\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     attention_mask = \u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;66;03m# If using a cache, it can give all information about mask sizes based on seen tokens\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, transcript in enumerate(merged_transcripts):\n",
    "    print(f\"Processing transcript {i+1}...\")\n",
    "    full_prompt = create_prompt(transcript)\n",
    "\n",
    "    outputs = extractor(full_prompt, max_new_tokens=500, return_full_text=False, do_sample=False)\n",
    "\n",
    "    generated_text = outputs[0]['generated_text']\n",
    "\n",
    "    results.append({\"original_transcript\": transcript,\n",
    "                    \"extraction\": generated_text})\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('reason_step.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(results)} results to reason_step.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d49dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript 1...\n",
      "Processing transcript 2...\n",
      "Processing transcript 3...\n",
      "Processing transcript 4...\n",
      "Processing transcript 5...\n",
      "Processing transcript 6...\n",
      "Processing transcript 7...\n",
      "Processing transcript 8...\n",
      "Processing transcript 9...\n",
      "Processing transcript 10...\n",
      "Processing transcript 11...\n",
      "Processing transcript 12...\n",
      "Processing transcript 13...\n",
      "Processing transcript 14...\n",
      "Processing transcript 15...\n",
      "Processing transcript 16...\n",
      "Processing transcript 17...\n",
      "Processing transcript 18...\n",
      "Processing transcript 19...\n",
      "Processing transcript 20...\n",
      "Processing transcript 21...\n",
      "Processing transcript 22...\n",
      "Processing transcript 23...\n",
      "Processing transcript 24...\n",
      "Processing transcript 25...\n",
      "Processing transcript 26...\n",
      "Processing transcript 27...\n",
      "Processing transcript 28...\n",
      "Processing transcript 29...\n",
      "Processing transcript 30...\n",
      "Processing transcript 31...\n",
      "Processing transcript 32...\n",
      "Processing transcript 33...\n",
      "Processing transcript 34...\n",
      "Processing transcript 35...\n",
      "Processing transcript 36...\n",
      "Processing transcript 37...\n",
      "Processing transcript 38...\n",
      "Processing transcript 39...\n",
      "Processing transcript 40...\n",
      "Processing transcript 41...\n",
      "Processing transcript 42...\n",
      "Processing transcript 43...\n",
      "Processing transcript 44...\n",
      "Processing transcript 45...\n",
      "Processing transcript 46...\n",
      "Processing transcript 47...\n",
      "Processing transcript 48...\n",
      "Processing transcript 49...\n",
      "Processing transcript 50...\n",
      "Processing transcript 51...\n",
      "Processing transcript 52...\n",
      "Processing transcript 53...\n",
      "Processing transcript 54...\n",
      "Processing transcript 55...\n",
      "Processing transcript 56...\n",
      "Processing transcript 57...\n",
      "Processing transcript 58...\n",
      "Processing transcript 59...\n",
      "Processing transcript 60...\n",
      "Processing transcript 61...\n",
      "Processing transcript 62...\n",
      "Processing transcript 63...\n",
      "Processing transcript 64...\n",
      "Processing transcript 65...\n",
      "Processing transcript 66...\n",
      "Processing transcript 67...\n",
      "Processing transcript 68...\n",
      "Processing transcript 69...\n",
      "Processing transcript 70...\n",
      "Processing transcript 71...\n",
      "Processing transcript 72...\n",
      "Processing transcript 73...\n",
      "Processing transcript 74...\n",
      "Processing transcript 75...\n",
      "Saved 75 results to reason_step_openai.json\n"
     ]
    }
   ],
   "source": [
    "##OpenAI version\n",
    "client = OpenAI()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, transcript in enumerate(merged_transcripts):\n",
    "    print(f\"Processing transcript {i+1}...\")\n",
    "    full_prompt = create_prompt(transcript)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano-2025-08-07\",  \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generated_text = response.choices[0].message.content\n",
    "\n",
    "    results.append({\n",
    "        \"original_transcript\": transcript,\n",
    "        \"extraction\": generated_text\n",
    "    })\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('reason_step_openai.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(results)} results to reason_step_openai.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1930dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"reason\": \"The customer wanted to set up an automated birthday gift for their sponsored child, as no birthday gift automation was currently configured and the customer believed it should already be in place.\",\\n  \"steps\": [\\n    \"Greeted the customer and collected sponsor information, then verified the account name (Kim Reeves) and sponsor number.\",\\n    \"Opened the account and reviewed gift setup, finding only the monthly sponsorship automatic withdrawal and no automated birthday gift.\",\\n    \"Explained that a birthday gift automation was not configured and discussed the need to set one up.\",\\n    \"Verified the payment method on file (Visa ending in 5252) with the customer.\",\\n    \"Configured an automated birthday gift of $40 annually and processed a one-time birthday gift today for the missed date.\",\\n    \"Discussed timing and recipient details: gifts take 2–3 months to reach the child, clarified which child (Peru and Rwanda) and reviewed past gift activity (one automated gift existed for Rwanda in May 2023, but not as automation for birthdays).\",\\n    \"Confirmed that the automated birthday gift is now active and checked whether any further assistance was needed.\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d17b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
